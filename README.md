```markdown
# ðŸ› ï¸ AI Agent Server Template

This project provides a scalable, production-ready framework to **serve AI agents via API** using [FastAPI](https://fastapi.tiangolo.com/), [LangServe](https://docs.langchain.dev/langserve/), and [LangChain](https://www.langchain.dev/).

It is designed to support **multi-agent systems**, clean modularization, and future extensibility (e.g., pluggable chat histories, multiple LLM providers).

---

## ðŸš€ Quickstart

1. **Install dependencies**

```bash
pdm install
```

2. **Run the server**

```bash
pdm run python -m app.server
```
or 
```bash
pdm run langchain serve
```


> The API will be available at:
> ```
> http://localhost:8000
> ```

---

## ðŸ§© Project Structure

```
. â”œâ”€â”€ app/ â”‚ â”œâ”€â”€ server.py # FastAPI server entrypoint â”‚ â”œâ”€â”€ routes.py # API routes setup (connect chains to API) â”‚ â”œâ”€â”€ utils.py # Request config modifiers and shared helpers â”‚ â”œâ”€â”€ client.ipynb # Client example notebook (for testing APIs) â”‚ â””â”€â”€ init.py â”‚ â”œâ”€â”€ chat_histories/ # (Temporary) Local chat history storage (file-based) â”‚ â””â”€â”€ user_id/ # Conversation history files (one JSON per session) â”‚ â”œâ”€â”€ core/ â”‚ â”œâ”€â”€ config.py # Load environment variables and settings â”‚ â”œâ”€â”€ model.py # Model loader (e.g., AzureChatOpenAI, OpenAI) â”‚ â”œâ”€â”€ tools.py # Shared tools across agents â”‚ â”œâ”€â”€ schemas.py # Input/Output data schemas (TypedDicts, etc.) â”‚ â”œâ”€â”€ agent_call.py # Core agent orchestration logic â”‚ â”œâ”€â”€ adapters/ â”‚ â”‚ â””â”€â”€ chat_history/ â”‚ â”‚ â””â”€â”€ file_history.py # Local file-based chat history adapter â”‚ â”œâ”€â”€ agents/ # Agents implementation â”‚ â”‚ â”œâ”€â”€ pirate_agent/ â”‚ â”‚ â”‚ â”œâ”€â”€ pirate.py # Pirate agent logic â”‚ â”‚ â”‚ â”œâ”€â”€ prompt.py # Prompts specific to pirate agent â”‚ â”‚ â”‚ â””â”€â”€ init.py â”‚ â”‚ â”œâ”€â”€ agent2/ â”‚ â”‚ â”‚ â””â”€â”€ init.py # Placeholder for another agent â”‚ â”‚ â””â”€â”€ init.py â”‚ â””â”€â”€ init.py â”‚ â”œâ”€â”€ pyproject.toml # Project and dependency configuration (PDM) â”œâ”€â”€ pdm.lock # Lock file for dependencies â””â”€â”€ README.md # Project documentation
```

---

## âœ¨ Concept Design

| Layer | Responsibility |
|:---|:---|
| `app/` | API serving (FastAPI app, route binding, request schemas, adapters) |
| `core/` | Business logic (chains, prompts, models, agents, tools) |

- `app/` is **lightweight and stateless**.
- `core/` contains **all AI/agent logic**.
- **Adapters** allow flexible backend storage (e.g., chat history can swap from file to database/API easily).
- **Agents** are isolated under `core/agents/` for clean multi-agent support.

---

## ðŸ“¦ Tech Stack

- [FastAPI](https://fastapi.tiangolo.com/) â€” high-performance API framework
- [LangServe](https://docs.langchain.dev/langserve/) â€” serve LangChain Runnables via API
- [LangChain](https://www.langchain.dev/) â€” LLM application framework
- [PDM](https://pdm.fming.dev/latest/) â€” Python dependency manager
- [Azure OpenAI / OpenAI API](https://learn.microsoft.com/en-us/azure/ai-services/openai/) â€” LLM provider

---

## ðŸ”® Future Plans

- **Chat History**:  
  - Currently using **local file storage**.
  - Easy to switch to **API backend** or **database backend** via adapter pattern.
  
- **Multi-Agent Orchestration**:  
  - Support running multiple agents, each with its own chain, prompts, and logic.
  
- **Monitoring and Logging**:  
  - Add tracing and logging to track API usage and agent conversations.

---

## âš¡ How to Add a New Agent

1. Create a new folder inside `core/agents/`, e.g., `agent3/`
2. Define your:
    - `agent.py` â€” main agent logic
    - `prompt.py` â€” prompts specific to the agent
    - `node.py` â€” LangGraph nodes (if needed)
3. Wrap your agent's chain with `wrap_chain_with_history()` from `core/chain/utils.py`
4. Register the agent in `core/setup.py` or make a new route if needed.

